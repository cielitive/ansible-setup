# ==========
# 【参考】環境情報
# ==========
example.com:
  network: 192.168.3.0/24
  host:
    - "k8s-master":
        os: Red Hat Enterprise Linux 8.10 (Ootpa)
        IP:
          enp0s3: -
          enp0s8(hostonly): 192.168.56.101
          enp0s9(bridge): 192.168.3.123
        DNS: -
        GW: 192.168.3.1
    - "k8s-node01":
        OS: Red Hat Enterprise Linux 8.10 (Ootpa)
        IP:
          enp0s3: -
          enp0s8(hostonly): 192.168.56.xxx
          enp0s9(bridge): 192.168.3.xxx
        DNS: -
        GW: 192.168.3.1

Kubernetes:
  Version: 1.30.0
Docker:
  Engine:
    Version: 26.1.4
  containerd:
    Version: 1.6.33
  runc:
    Version: 1.1.12
CNI:
  Calico:
    Version: 3.28.0
Prometeus:
  Version: 
Grafana:
  Version: 

# ==========
# 事前準備 ( ControlePlane, WokerNode )
# ==========
# -----
# [参考]自動接続有効化(=IPアドレス自動付与)
# -----
$ nmcli connection modify enp0s8 connection.autoconnect yes
$ nmcli connection modify enp0s9 connection.autoconnect yes

# -----
# [参考]サブスクリプションチェック無効化
# -----
$ sed -i 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/subscription-manager.conf

# -----
# [参考]ローカルDNFリポジトリ作成
# -----
### isoファイルは「/root」配下に配置する

$ cat <<EOT >/etc/yum.repos.d/local.repo
[BaseOS-media]
name=Red Hat Enterprise Linux 8 - BaseOS
# metadata_expire=-1
gpgcheck=1
enabled=1
baseurl=file:///mnt/BaseOS/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

[AppStream-media]
name=Red Hat Enterprise Linux 8 - AppStream
# metadata_expire=-1
gpgcheck=1
enabled=1
baseurl=file:///mnt/AppStream/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
EOT

$ mount -o loop,ro,auto,nofail -t iso9660 /root/rhel-8.10-x86_64-dvd.iso /mnt
$ echo "/root/rhel-8.10-x86_64-dvd.iso   /mnt   iso9660   loop,ro,auto,nofail 0 0" >>/etc/fstab 

# -----
# [参考]ユーザ作成
# -----

# -----
# ホスト名変更
# -----
$ hostnamectl set-hostname k8s-master

# -----
# "hosts"追加
# -----
$ sudo sed -i '/^127.0.1.1/d' /etc/hosts
$ sudo cat <<EOT >>/etc/hosts
192.168.56.101   k8s-master.example.com k8s-master
EOT

# -----
# swap無効化
# -----
$ sudo swapoff -a
$ sudo sed -i -e '/swap/s/^/# /g' /etc/fstab

### swapが無効化されていることを確認する(→なにも表示されないこと)
$ sudo swapon -s

# -----
# Firewalld無効化
# -----
$ sudo systemctl disable --now firewalld
$ sudo systemctl is-active firewalld
inactive
$ sudo systemctl is-enabled firewalld
disabled

# -----
# SELinux無効化
# -----
$ sudo setenforce 0
$ sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
$ reboot

$ sestatus 
SELinux status:                 disabled

# -----
# [参考]"UUID"確認
# -----
$ dmidecode -s system-uuid
3A164042-9096-4421-AF53-4DB78638690A

# -----
# 前提パッケージ
# -----
$ dnf install -y vim git wget jq
$ dnf install -y iproute-tc chrony
$ rpm -qa | egrep "iproute-tc|chrony"
chrony-4.3-1.el9.aarch64
iproute-tc-6.2.0-5.el9.aarch64

# -----
# カーネルモジュール有効化
# -----
$ sudo modprobe overlay
$ sudo modprobe br_netfilter

### 永続化
$ cat <<EOT | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOT

$ lsmod | egrep "overlay|br_netfilter"
br_netfilter           32768  0
bridge                290816  1 br_netfilter
overlay               151552  0

# -----
# カーネルパラメータ修正
# -----
$ cat <<EOT | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOT

$ sudo sysctl --system
$ sudo sysctl net.ipv4.ip_forward net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1


# ==========
# Docker
# ==========
# -----
# Dockerリポジトリ追加
# -----
$ sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo

# -----
# "Docker"インストール
# -----
$ sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
$ rpm -qa | grep -E "docker-ce|docker-ce-cli|containerd.io"
containerd.io-1.7.18-3.1.el8.x86_64
docker-ce-cli-27.0.3-1.el8.x86_64
docker-ce-27.0.3-1.el8.x86_64
docker-ce-rootless-extras-27.0.3-1.el8.x86_64

# -----
# [参考]デーモン設定変更
# -----
# cat <<EOF > /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

# -----
# "Docker"起動
# -----
$ systemctl --now enable docker.service
$ systemctl is-active docker.service
active
$ systemctl is-enabled docker.service
enabled

# -----
# バージョン確認
# -----
$ docker version
Client: Docker Engine - Community
 Version:           26.1.4
 API version:       1.45
 Go version:        go1.21.11
 Git commit:        5650f9b
 Built:             Wed Jun  5 11:30:27 2024
 OS/Arch:           linux/arm64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          26.1.4
  API version:      1.45 (minimum version 1.24)
  Go version:       go1.21.11
  Git commit:       de5c9cf
  Built:            Wed Jun  5 11:29:15 2024
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          1.6.33
  GitCommit:        d2d58213f83a351ca8f528a95fbd145f5654e957
 runc:
  Version:          1.1.12
  GitCommit:        v1.1.12-0-g51d5e94
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

# -----
# [参考]ストレージ・ドライバ確認
# -----
$ docker info
Client: Docker Engine - Community
 Version:    26.1.4
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.14.1
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.27.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 26.1.4
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: d2d58213f83a351ca8f528a95fbd145f5654e957
 runc version: v1.1.12-0-g51d5e94
 init version: de40ad0
 Security Options:
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 5.14.0-362.18.1.el9_3.aarch64
 Operating System: Rocky Linux 9.3 (Blue Onyx)
 OSType: linux
 Architecture: aarch64
 CPUs: 2
 Total Memory: 3.502GiB
 Name: k8s-master.example.com
 ID: f8170aa4-406c-45a2-9686-5e0aff01ec74
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false

# -----
# コンテナランタイムの設定修正(Cgroup Driver)
# -----
$ cp -p /etc/containerd/config.toml /etc/containerd/config.toml_$(date +%Y%m%d)
$ containerd config default >/etc/containerd/config.toml
$ sed -i '/SystemdCgroup/s/false/true/' /etc/containerd/config.toml
$ grep "SystemdCgroup" /etc/containerd/config.toml
            SystemdCgroup = true

$ systemctl restart containerd.service docker.service

# -----
# [参考]Proxy設定
# -----
$ mkdir -p /etc/systemd/system/docker.service.d
$ cat > /etc/systemd/system/docker.service.d/http-proxy.conf <<EOT
[Service]
Environment="HTTP_PROXY=http://proxy.example.com:80/"
EOT

$ cat > /etc/systemd/system/docker.service.d/https-proxy.conf <<EOT
[Service]
Environment="HTTPS_PROXY=https://proxy.example.com:443/"
EOT


# ==========
# Kubernetes
# ==========
# -----
# Kubernetesリポジトリ追加
# -----
$ cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

# -----
# "Kubernetes"インストール
# -----
$ sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
$ systemctl enable kubelet.service

# -----
#  デフォルト設定修正
# -----
$ kubeadm config print init-defaults | tee ClusterConfiguration.yaml
$ cat <<EOT >>ClusterConfiguration.yaml
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
EOT

$ sed -i '/advertiseAddress/s/1.2.3.4/192.168.56.101/' ClusterConfiguration.yaml
$ sed -i '/^  name/s/node/k8s-master/' ClusterConfiguration.yaml
$ sed -i '/serviceSubnet/N;s/\n/\n  podSubnet: 10.0.0.0\/16\n/;' ClusterConfiguration.yaml

# [修正後] ----------------------------------------

apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.56.101
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: k8s-master
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io
kind: ClusterConfiguration
kubernetesVersion: 1.30.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.0.0.0/16
scheduler: {}
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd

# [修正後] ----------------------------------------

# -----
# 
# -----
$ kubeadm config print join-defaults | tee JoinConfiguration.yaml

# -----
# 
# -----
$ kubeadm init --config=ClusterConfiguration.yaml
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.56.101:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:a4a6b26fcb0a92f8a6ddf4cb815252a454b4ab5d0ccb7d1f8ab3a31574db4e38 

# -----
# [参考]ハッシュ値(sha256)再表示
# -----
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'

# -----
# 
# -----
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

# -----
# バージョン確認
# -----
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

# -----
# Workerノード登録
# -----
$ kubectl describe node k8s-master
Name:               k8s-master
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=k8s-master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 16 Jun 2024 20:19:43 -0500
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule

$ kubectl taint nodes k8s-master node-role.kubernetes.io/control-plane:NoSchedule-
node/k8s-master untainted

$ kubectl describe node k8s-master
Name:               k8s-master
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=k8s-master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 16 Jun 2024 20:19:43 -0500
Taints:             node.kubernetes.io/not-ready:NoSchedule

# -----
#
# -----
$ kubectl get node --all-namespaces
NAME         STATUS   ROLES           AGE   VERSION
k8s-master   Ready    control-plane   22m   v1.30.2

$ kubectl get nodes -o wide
NAME         STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                      KERNEL-VERSION                  CONTAINER-RUNTIME
k8s-master   Ready    control-plane   18m   v1.30.2   192.168.9.20   <none>        Rocky Linux 9.3 (Blue Onyx)   5.14.0-362.18.1.el9_3.aarch64   containerd://1.6.33


# ==========
# Flannel ( ControlePlane, WokerNode )
# ==========
# -----
#
# -----


# ==========
# Calico
# ==========
# -----
#
# -----
$ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml

# -----
#
# -----
$ mkdir ~/calico && cd $_
$ curl -OL https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml
$ sed -i 's/cidr: 192.168.0.0/cidr: 10.0.0.0/' custom-resources.yaml
$ kubectl create -f custom-resources.yaml
installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created

# -----
#
# -----
$ kubectl get namespace --all-namespaces
NAME               STATUS   AGE
calico-apiserver   Active   2m44s
calico-system      Active   3m47s
default            Active   21m
kube-node-lease    Active   21m
kube-public        Active   21m
kube-system        Active   21m
tigera-operator    Active   5m19s

# -----
#
# -----
$ kubectl get pod --all-namespaces
NAMESPACE          NAME                                      READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-85658c95f8-qvmjn         1/1     Running   0          3m42s
calico-apiserver   calico-apiserver-85658c95f8-z8dbv         1/1     Running   0          3m42s
calico-system      calico-kube-controllers-b566bbb67-v45qw   1/1     Running   0          4m45s
calico-system      calico-node-qjdqd                         1/1     Running   0          4m45s
calico-system      calico-typha-7cbc7d4d58-4qbvm             1/1     Running   0          4m45s
calico-system      csi-node-driver-4g47q                     2/2     Running   0          4m45s
kube-system        coredns-7db6d8ff4d-8pbmf                  1/1     Running   0          21m
kube-system        coredns-7db6d8ff4d-kghzf                  1/1     Running   0          21m
kube-system        etcd-k8s-master                           1/1     Running   0          22m
kube-system        kube-apiserver-k8s-master                 1/1     Running   0          22m
kube-system        kube-controller-manager-k8s-master        1/1     Running   0          22m
kube-system        kube-proxy-mdvvf                          1/1     Running   0          21m
kube-system        kube-scheduler-k8s-master                 1/1     Running   0          22m
tigera-operator    tigera-operator-76ff79f7fd-4vrjf          1/1     Running   0          6m16s

# -----
#
# -----
$ kubectl get service --all-namespaces
NAMESPACE          NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
calico-apiserver   calico-api                        ClusterIP   10.96.172.216    <none>        443/TCP                  4m12s
calico-system      calico-kube-controllers-metrics   ClusterIP   None             <none>        9094/TCP                 4m19s
calico-system      calico-typha                      ClusterIP   10.106.186.168   <none>        5473/TCP                 5m15s
default            kubernetes                        ClusterIP   10.96.0.1        <none>        443/TCP                  22m
kube-system        kube-dns                          ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   22m

# -----
#
# -----
$ kubectl get daemonset --all-namespaces
NAMESPACE       NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
calico-system   calico-node       1         1         1       1            1           kubernetes.io/os=linux   5m41s
calico-system   csi-node-driver   1         1         1       1            1           kubernetes.io/os=linux   5m41s
kube-system     kube-proxy        1         1         1       1            1           kubernetes.io/os=linux   23m


# ==========
# サンプルアプリケーション
# ==========
# -----
#
# -----
$ mkdir /root/nginx && cd $_
$ cat <<EOT | sudo tee nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
EOT

# -----
#
# -----
$ kubectl create -f nginx.yaml
deployment "nginx-deployment" created

# -----
#
# -----
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   2/2     2            2           12s

# -----
#
# -----
$ kubectl get replicasets
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-77d8468669   2         2         2       29s

# -----
#
# -----
# kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-77d8468669-gwnhk   1/1     Running   0          41s
nginx-deployment-77d8468669-h8vgl   1/1     Running   0          41s

# -----
#
# -----
# kubectl describe pods nginx-deployment

# -----
#
# -----
$ kubectl expose deployment nginx-deployment --type=NodePort --name=nginx-service --port=80 --target-port=80
service/nginx-service exposed

# -----
#
# -----
$ kubectl get service
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes      ClusterIP   10.96.0.1        <none>        443/TCP        15h
nginx-service   NodePort    10.100.104.214   <none>        80:31116/TCP   9s

# -----
#
# -----
$ kubectl describe services nginx-service
Name:                     nginx-service
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.107.84.112
IPs:                      10.107.84.112
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32352/TCP
Endpoints:                10.0.235.199:80,10.0.235.200:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

# -----
#
# -----
### "Welcome to nginx!"が表示されれば問題なし
$ curl -s $(kubectl get service | grep nginx-service | awk '{print $3}') | grep "title"

# -----
# [参考]Service削除
# -----
# kubectl delete service nginx-service

# -----
# [参考]deployment削除
# -----
# kubectl delete deployment nginx-deployment


# ==========
# Prometheus
# ==========
# -----
#
# -----
$ cd /root
$ git clone https://github.com/coreos/kube-prometheus.git && cd kube-prometheus
$ kubectl apply --server-side -f manifests/setup
$ kubectl wait \
  --for condition=Established \
  --all CustomResourceDefinition \
  --namespace=monitoring
$ kubectl apply -f manifests/

# -----
#
# -----
$ kubectl get pod --namespace monitoring
NAME                                  READY   STATUS    RESTARTS   AGE
alertmanager-main-0                   2/2     Running   0          8m24s
alertmanager-main-1                   2/2     Running   0          8m24s
alertmanager-main-2                   2/2     Running   0          8m24s
blackbox-exporter-597d86cf5c-n524m    3/3     Running   0          9m25s
grafana-854bdcdf45-v7h64              1/1     Running   0          9m24s
kube-state-metrics-6b6c9cf9f6-x5jpm   3/3     Running   0          9m24s
node-exporter-ftgmv                   2/2     Running   0          9m23s
prometheus-adapter-5794d7d9f5-bzbgb   1/1     Running   0          9m22s
prometheus-adapter-5794d7d9f5-prpgl   1/1     Running   0          9m22s
prometheus-k8s-0                      2/2     Running   0          8m23s
prometheus-k8s-1                      2/2     Running   0          8m23s
prometheus-operator-b544c4b97-nbrcn   2/2     Running   0          9m22s

# -----
#
# -----
$ kubectl get service --namespace monitoring
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.107.225.222   <none>        9093/TCP,8080/TCP            9m59s
alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   8m58s
blackbox-exporter       ClusterIP   10.103.15.40     <none>        9115/TCP,19115/TCP           9m58s
grafana                 ClusterIP   10.100.181.95    <none>        3000/TCP                     9m57s
kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP            9m57s
node-exporter           ClusterIP   None             <none>        9100/TCP                     9m56s
prometheus-adapter      ClusterIP   10.97.129.35     <none>        443/TCP                      9m55s
prometheus-k8s          ClusterIP   10.99.17.160     <none>        9090/TCP,8080/TCP            9m56s
prometheus-operated     ClusterIP   None             <none>        9090/TCP                     8m56s
prometheus-operator     ClusterIP   None             <none>        8443/TCP                     9m55s
